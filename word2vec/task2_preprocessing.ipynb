{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing is not the most exciting part of NLP, but it is still one of the most important ones. Your task is to preprocess raw text (you can use your own, or [this one](http://mattmahoney.net/dc/text8.zip). For this task text preprocessing mostly consists of:\n",
    "\n",
    "1. cleaning (mostly, if your dataset is from social media or parsed from the internet)\n",
    "1. tokenization\n",
    "1. building the vocabulary and choosing its size. Use only high-frequency words, change all other words to UNK or handle it in your own manner. You can use `collections.Counter` for that.\n",
    "1. assigning each token a number (numericalization). In other words, make word2index Ð¸ index2word objects.\n",
    "1. data structuring and batching - make X and y matrices generator for word2vec (explained in more details below)\n",
    "\n",
    "**ATTN!:** If you use your own data, please, attach a download link. \n",
    "\n",
    "Your goal is to make SkipGramBatcher class which returns two numpy tensors with word indices. It should be possible to use one for word2vec training. You can implement batcher for Skip-Gram or CBOW architecture, the picture below can be helpful to remember the difference.\n",
    "\n",
    "![text](https://raw.githubusercontent.com/deepmipt/deep-nlp-seminars/651804899d05b96fc72b9474404fab330365ca09/seminar_02/pics/architecture.png)\n",
    "\n",
    "There are several ways to do it right. Shapes could be `x_batch.shape = (batch_size, 2*window_size)`, `y_batch.shape = (batch_size,)` for CBOW or `(batch_size,)`, `(batch_size,)` for Skip-Gram. You should **not** do negative sampling here.\n",
    "\n",
    "They should be adequately parametrized: CBOW(window_size, ...), SkipGram(window_size, ...). You should implement only one batcher in this task; and it's up to you which one to chose.\n",
    "\n",
    "Useful links:\n",
    "1. [Word2Vec Tutorial - The Skip-Gram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
    "1. [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)\n",
    "1. [Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
    "\n",
    "You can write the code in this notebook, or in a separate file. It can be reused for the next task. The result of your work should represent that your batch has a proper structure (right shapes) and content (words should be from one context, not some random indices). To show that, translate indices back to words and print them to show something like this:\n",
    "\n",
    "```\n",
    "text = ['first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including']\n",
    "\n",
    "window_size = 2\n",
    "\n",
    "# CBOW:\n",
    "indices_to_words(x_batch) = \\\n",
    "        [['first', 'used', 'early', 'working'],\n",
    "        ['used', 'against', 'working', 'class'],\n",
    "        ['against', 'early', 'class', 'radicals'],\n",
    "        ['early', 'working', 'radicals', 'including']]\n",
    "\n",
    "indices_to_words(labels_batch) = ['against', 'early', 'working', 'class']\n",
    "\n",
    "# Skip-Gram\n",
    "\n",
    "indices_to_words(x_batch) = ['against', 'early', 'working', 'class']\n",
    "\n",
    "indices_to_words(labels_batch) = ['used', 'working', 'early', 'radicals']]\n",
    "\n",
    "```\n",
    "\n",
    "If you struggle with something, ask your neighbor. If it is not obvious for you, probably someone else is looking for the answer too. And in contrast, if you see that you can help someone - do it! Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!wget http://mattmahoney.net/dc/text8.zip\n",
    "\n",
    "!ls\n",
    "!unzip text8.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17005208\n"
     ]
    }
   ],
   "source": [
    "filename = 'text8' #file with the raw text\n",
    "text = []\n",
    "with open('text8', mode='r') as file:\n",
    "    line = file.readline()\n",
    "    while line:\n",
    "        text += line.lower().split(' ')\n",
    "        line = file.readline()\n",
    "        if len(text) > 100000:\n",
    "            print(len(text))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17005208\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(text))\n",
    "train_text = text[:1000]\n",
    "print(len(train_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Note that in our case, the text8 model already has bare text, that we only have to care about lowercase and frequency\"\"\"\n",
    "unknown_token = \"UNK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"batcher class for the model\"\"\"\n",
    "class SkipGramBatcher:\n",
    "    def __init__(self, window_size=5, least_freq=3):\n",
    "        self.least_freq = least_freq\n",
    "        self.text = None\n",
    "        self.vocab = None\n",
    "        self.vocab_size = None\n",
    "        self.word2index = None\n",
    "        self.index2word = None\n",
    "        self.window_size = window_size\n",
    "        self.current_index = 0\n",
    "        self.current_diff = -window_size\n",
    "        self.total_size = 0\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"replace words with frequency < least_freq with unknown_token\n",
    "        and save the text\n",
    "        \"\"\"\n",
    "        counter = Counter(text)\n",
    "        def get_token(word):\n",
    "            if counter[word] < self.least_freq:\n",
    "                return unknown_token\n",
    "            else:\n",
    "                return word.lower()\n",
    "        self.text = [get_token(word) for word in text]\n",
    "    \n",
    "    def fit_text(self, text):\n",
    "        \"\"\"init text, vocab, word2ind, ind2word\n",
    "        \"\"\"\n",
    "        self.preprocess(text)\n",
    "        self.vocab = np.unique(self.text)\n",
    "        self.vocab_size = self.vocab.shape[0]\n",
    "        self.word2index = dict(zip(self.vocab, range(self.vocab.shape[0])))\n",
    "        self.index2word = dict(zip(range(self.vocab.shape[0]), self.vocab))\n",
    "        self.total_size = (len(self.text) - 3 * self.window_size) * self.window_size * 2\n",
    "        \n",
    "    def most_frequent(self, num=25):\n",
    "        \"\"\"get most frequent words from the text\"\"\"\n",
    "        counter = Counter(self.text)\n",
    "        return counter.most_common(num)\n",
    "        \n",
    "    def least_frequent(self, num=25):\n",
    "        \"\"\"get least frequent words from the text\"\"\"\n",
    "        counter = Counter(self.text)\n",
    "        return counter.most_common()[:-num - 1:-1]\n",
    "        \n",
    "    def indices_to_words(self, x_batch):\n",
    "        \"\"\"return array of words out of array of indices\"\"\"\n",
    "        return np.array([self.index2word[index] for index in x_batch])\n",
    "    \n",
    "    def words_to_indices(self, words):\n",
    "        \"\"\"return array of indices out of array of words\"\"\"\n",
    "        return np.array([self.word2index[word] for word in words])\n",
    "    \n",
    "    def _get_next_index_and_diff(self, current_index, current_diff):\n",
    "        if (current_diff == self.window_size):\n",
    "            current_diff = -self.window_size\n",
    "            current_index += 1\n",
    "            current_index %= len(self.text)\n",
    "        else:\n",
    "            if current_diff == -1:\n",
    "                current_diff = 1\n",
    "            else:\n",
    "                current_diff += 1\n",
    "        return current_index, current_diff\n",
    "\n",
    "    \n",
    "    def get_batch(self, batch_size=100):\n",
    "        \"\"\"return batch of indices for x and for labels consequently\"\"\"\n",
    "        x_batch = []\n",
    "        labels_batch = []\n",
    "        while len(x_batch) < batch_size:\n",
    "            label_index_in_text = self.current_index + self.current_diff \n",
    "            if (label_index_in_text < 0 or label_index_in_text >= len(self.text)):\n",
    "                index, diff = self._get_next_index_and_diff(self.current_index, self.current_diff)\n",
    "                self.current_index = index\n",
    "                self.current_diff = diff\n",
    "                continue\n",
    "                \n",
    "            word = self.text[self.current_index]\n",
    "            word_index = self.word2index[word]\n",
    "            label = self.text[self.current_index + self.current_diff]\n",
    "            label_index = self.word2index[label]\n",
    "            \n",
    "            x_batch.append(word_index)\n",
    "            labels_batch.append(label_index)\n",
    "            \n",
    "            index, diff = self._get_next_index_and_diff(self.current_index, self.current_diff)\n",
    "            self.current_index = index\n",
    "            self.current_diff = diff\n",
    "            \n",
    "        assert len(x_batch) == batch_size\n",
    "        assert len(labels_batch) == batch_size\n",
    "        \n",
    "        x_batch = np.array(x_batch)\n",
    "        labels_batch = np.array(labels_batch)\n",
    "        \n",
    "        permut = np.random.permutation(range(batch_size))\n",
    "        x_batch = x_batch[permut]\n",
    "        labels_batch = labels_batch[permut]\n",
    "        return x_batch, labels_batch\n",
    "      \n",
    "    def get_random_batch(self, batch_size=100):\n",
    "        \"\"\"return batch of indices for x and for labels randomly\"\"\"\n",
    "        x_batch = []\n",
    "        labels_batch = []\n",
    "        \n",
    "        indices = np.random.choice(np.arange(self.window_size + 100, len(self.text) - self.window_size - 100), batch_size, replace=False)\n",
    "        words = itemgetter(*indices.tolist())(self.text)\n",
    "        x_batch = [self.word2index[word] for word in words]\n",
    "        \n",
    "        diffs = np.random.randint(-self.window_size, +self.window_size, size=batch_size)\n",
    "        label_indices = indices + diffs\n",
    "        labels = itemgetter(*label_indices.tolist())(self.text)\n",
    "        labels_batch = [self.word2index[label] for label in labels]\n",
    "        \n",
    "        x_batch = np.array(x_batch)\n",
    "        labels_batch = np.array(labels_batch)\n",
    "        return x_batch, labels_batch\n",
    "        \n",
    "    def batch_generator(self, batch_size=100):\n",
    "        \"\"\"generator for batch\"\"\"\n",
    "        while True:\n",
    "            x_batch, labels_batch = self.get_batch(batch_size)\n",
    "            yield x_batch, labels_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"lets visualize the process\"\"\"\n",
    "skpgram_batcher = SkipGramBatcher(window_size=2, least_freq=2)\n",
    "skpgram_batcher.fit_text(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my\n",
      "my\n"
     ]
    }
   ],
   "source": [
    "index = random.randint(0, skpgram_batcher.vocab_size - 1)\n",
    "word = skpgram_batcher.index2word[index]\n",
    "print(skpgram_batcher.index2word[index])\n",
    "print(skpgram_batcher.index2word[skpgram_batcher.word2index[word]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SHAPE:  (152,)\n",
      "['UNK' 'a' 'about' 'abuse' 'accepted' 'access' 'advocate' 'against' 'all'\n",
      " 'also' 'although' 'am' 'american' 'an' 'anabaptists' 'anarchism'\n",
      " 'anarchist' 'anarchists' 'anarchy' 'and' 'are' 'as' 'at' 'authoritarian'\n",
      " 'be']\n",
      "MOST FREQUENT WORDS:  [('UNK', 291), ('the', 58), ('of', 41), ('in', 30), ('and', 27), ('to', 18), ('as', 17), ('that', 15), ('is', 14), ('a', 12), ('anarchist', 10), ('property', 10), ('anarchism', 9), ('society', 9), ('are', 9), ('his', 9), ('it', 8), ('what', 8), ('an', 8), ('proudhon', 8), ('anarchists', 7), ('this', 7), ('he', 7), ('be', 6), ('was', 6)]\n"
     ]
    }
   ],
   "source": [
    "print('VOCAB SHAPE: ', skpgram_batcher.vocab.shape)\n",
    "print(skpgram_batcher.vocab[:25])\n",
    "print('MOST FREQUENT WORDS: ', skpgram_batcher.most_frequent())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch, labels_batch = skpgram_batcher.get_batch(batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50,)\n",
      "(50,)\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(x_batch.shape)\n",
    "print(labels_batch.shape)\n",
    "print(type(x_batch), type(labels_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT SHAPE:  1000\n",
      "['UNK', 'anarchism', 'UNK', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'UNK', 'including', 'the', 'diggers', 'of', 'the', 'english', 'revolution', 'and', 'the', 'UNK']\n"
     ]
    }
   ],
   "source": [
    "print('TEXT SHAPE: ', len(skpgram_batcher.text))\n",
    "print(skpgram_batcher.text[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UNK' 'UNK' 'anarchism' 'anarchism' 'anarchism' 'UNK' 'UNK' 'UNK' 'UNK'\n",
      " 'as' 'as' 'as' 'as' 'a' 'a' 'a' 'a' 'term' 'term' 'term' 'term' 'of' 'of'\n",
      " 'of' 'of' 'abuse' 'abuse' 'abuse' 'abuse' 'first' 'first' 'first' 'first'\n",
      " 'used' 'used' 'used' 'used' 'against' 'against' 'against' 'against'\n",
      " 'early' 'early' 'early' 'early' 'working' 'working' 'working' 'working'\n",
      " 'class']\n",
      "['anarchism' 'UNK' 'UNK' 'UNK' 'as' 'UNK' 'anarchism' 'as' 'a' 'anarchism'\n",
      " 'UNK' 'a' 'term' 'UNK' 'as' 'term' 'of' 'as' 'a' 'of' 'abuse' 'a' 'term'\n",
      " 'abuse' 'first' 'term' 'of' 'first' 'used' 'of' 'abuse' 'used' 'against'\n",
      " 'abuse' 'first' 'against' 'early' 'first' 'used' 'early' 'working' 'used'\n",
      " 'against' 'working' 'class' 'against' 'early' 'class' 'UNK' 'early']\n"
     ]
    }
   ],
   "source": [
    "print(skpgram_batcher.indices_to_words(x_batch))\n",
    "print(skpgram_batcher.indices_to_words(labels_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
